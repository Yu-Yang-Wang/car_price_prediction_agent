"""RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒå®ç°"""

import os
import sys
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

from .vector_store import VectorStoreManager
from .embeddings import EmbeddingManager
from database.manager import DatabaseManager
from car_analysis.graph.graph_service import GraphService

logger = logging.getLogger(__name__)


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self,
                 db_manager: Optional[DatabaseManager] = None,
                 vector_manager: Optional[VectorStoreManager] = None,
                 embedding_manager: Optional[EmbeddingManager] = None,
                 llm_model: str = "gpt-4o",
                 temperature: float = 0.3):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            vector_manager: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            embedding_manager: åµŒå…¥ç®¡ç†å™¨
            llm_model: LLMæ¨¡å‹åç§°
            temperature: LLMæ¸©åº¦å‚æ•°
        """
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_manager = db_manager or DatabaseManager()
        self.embedding_manager = embedding_manager or EmbeddingManager()
        self.vector_manager = vector_manager or VectorStoreManager(
            embedding_manager=self.embedding_manager
        )

        # åˆå§‹åŒ–LLM
        try:
            self.llm = ChatOpenAI(
                model=llm_model,
                temperature=temperature
            )
            print(f"ğŸ¤– RAG System initialized with {llm_model}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            raise

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self._create_prompt_templates()

        # åˆå§‹åŒ–å›¾æœåŠ¡ï¼ˆå¯é€‰ï¼‰
        try:
            self.graph_service = GraphService()
            if not self.graph_service.available:
                self.graph_service = None
                print("â„¹ï¸ GraphService not available; continuing without GraphRAG")
            else:
                print("ğŸ•¸ï¸ GraphRAG enabled")
        except Exception as ge:  # pragma: no cover
            logger.warning(f"GraphService init failed: {ge}")
            self.graph_service = None

    def _create_prompt_templates(self):
        """åˆ›å»ºå„ç§æç¤ºæ¨¡æ¿"""

        # æ±½è½¦åˆ†æå¢å¼ºæç¤º
        self.car_analysis_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ±½è½¦åˆ†æä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æ±½è½¦å¸‚åœºçŸ¥è¯†å’Œä»·æ ¼è¯„ä¼°ç»éªŒã€‚

ä½ æœ‰access toä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š
{retrieved_context}

è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¹ä»¥ä¸‹æ±½è½¦è¿›è¡Œæ·±å…¥åˆ†æã€‚

åˆ†æè¦æ±‚ï¼š
1. ç»¼åˆè€ƒè™‘å¸‚åœºæ•°æ®ã€ç±»ä¼¼è½¦è¾†åˆ†æã€å†å²ä»·æ ¼è¶‹åŠ¿
2. æä¾›ä¸“ä¸šçš„ä»·æ ¼è¯„ä¼°å’Œäº¤æ˜“å»ºè®®
3. è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹
4. å¦‚æœå‘ç°çŸ›ç›¾æˆ–ä¸ä¸€è‡´çš„ä¿¡æ¯ï¼Œè¯·æŒ‡å‡ºå¹¶ç»™å‡ºåˆç†è§£é‡Š
5. è€ƒè™‘è½¦è¾†çš„å“ç‰Œå¯é æ€§ã€ä¿å€¼ç‡ã€ç»´ä¿®æˆæœ¬ç­‰å› ç´ 

è¯·ç”¨æ¸…æ™°ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›å¤ã€‚
"""),
            HumanMessagePromptTemplate.from_template("{query}")
        ])

        # çŸ¥è¯†é—®ç­”æ¨¡æ¿
        self.qa_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½æ±½è½¦ä¸“å®¶ï¼Œå¯ä»¥å›ç­”å…³äºæ±½è½¦è´­ä¹°ã€å¸‚åœºåˆ†æã€ä»·æ ¼è¯„ä¼°ç­‰å„ç§é—®é¢˜ã€‚

ç›¸å…³å‚è€ƒä¿¡æ¯ï¼š
{retrieved_context}

è¯·åŸºäºæä¾›çš„ä¿¡æ¯å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜éœ€è¦æ›´å¤šä¿¡æ¯ã€‚
"""),
            HumanMessagePromptTemplate.from_template("{query}")
        ])

        # ç›¸ä¼¼æ¡ˆä¾‹åˆ†ææ¨¡æ¿
        self.similar_cases_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½æ±½è½¦ä»·æ ¼åˆ†æä¸“å®¶ã€‚ç”¨æˆ·è¯¢é—®å…³äºç‰¹å®šè½¦è¾†çš„ä¿¡æ¯ï¼Œä½ éœ€è¦åŸºäºç›¸ä¼¼è½¦è¾†çš„å†å²åˆ†ææ¥æä¾›æ´å¯Ÿã€‚

ç›¸ä¼¼æ¡ˆä¾‹åˆ†æï¼š
{retrieved_context}

è¯·åˆ†æè¿™äº›ç›¸ä¼¼æ¡ˆä¾‹ï¼Œæ€»ç»“è§„å¾‹å’Œè¶‹åŠ¿ï¼Œä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„å‚è€ƒä¿¡æ¯ã€‚
"""),
            HumanMessagePromptTemplate.from_template("{query}")
        ])

    def enhance_car_analysis(self,
                           car_data: Dict[str, Any],
                           analysis_context: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨RAGå¢å¼ºæ±½è½¦åˆ†æ

        Args:
            car_data: æ±½è½¦æ•°æ®
            analysis_context: åˆ†æä¸Šä¸‹æ–‡

        Returns:
            å¢å¼ºçš„åˆ†æç»“æœ
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼ˆGraphRAG å­å›¾ + å‘é‡æ£€ç´¢èåˆï¼‰
            retrieved_info = self._retrieve_for_car_analysis(car_data)

            # 2. æ„å»ºæŸ¥è¯¢
            car_description = self.embedding_manager.create_car_description(car_data)
            query = f"""
è¯·åˆ†æä»¥ä¸‹è½¦è¾†ï¼š
{car_description}

å·²çŸ¥ä¿¡æ¯ï¼š
- è´­ä¹°ä»·æ ¼: ${car_data.get('price_paid', 0):,.0f}
- é‡Œç¨‹æ•°: {car_data.get('mileage', 0):,} miles

{analysis_context if analysis_context else ''}

è¯·æä¾›è¯¦ç»†çš„ä»·æ ¼åˆ†æå’Œè´­ä¹°å»ºè®®ã€‚
"""

            # 3. ç”Ÿæˆå¢å¼ºå›å¤
            enhanced_response = self._generate_response(
                template=self.car_analysis_template,
                query=query,
                retrieved_context=retrieved_info
            )

            return {
                "enhanced_analysis": enhanced_response,
                "retrieved_info": retrieved_info,
                "rag_confidence": self._calculate_confidence(retrieved_info)
            }

        except Exception as e:
            logger.error(f"Error in enhance_car_analysis: {e}")
            return {
                "enhanced_analysis": "æ— æ³•ç”Ÿæˆå¢å¼ºåˆ†æ",
                "error": str(e)
            }

    def answer_question(self, question: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä½¿ç”¨RAGå›ç­”ç”¨æˆ·é—®é¢˜

        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: é¢å¤–ä¸Šä¸‹æ–‡

        Returns:
            å›ç­”ç»“æœ
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³ä¿¡æ¯
            retrieved_info = self._retrieve_for_question(question)

            # 2. ç”Ÿæˆå›ç­”
            answer = self._generate_response(
                template=self.qa_template,
                query=question,
                retrieved_context=retrieved_info
            )

            # 3. ä¿å­˜ç”¨æˆ·æŸ¥è¯¢ï¼ˆç”¨äºæ”¹è¿›ç³»ç»Ÿï¼‰
            self._save_user_query(question, retrieved_info, answer, context)

            return {
                "answer": answer,
                "retrieved_info": retrieved_info,
                "confidence": self._calculate_confidence(retrieved_info)
            }

        except Exception as e:
            logger.error(f"Error in answer_question: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚",
                "error": str(e)
            }

    def find_similar_cases(self,
                          car_data: Dict[str, Any],
                          analysis_type: str = "all") -> Dict[str, Any]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ¡ˆä¾‹å¹¶ç”Ÿæˆåˆ†æ

        Args:
            car_data: æ±½è½¦æ•°æ®
            analysis_type: åˆ†æç±»å‹

        Returns:
            ç›¸ä¼¼æ¡ˆä¾‹åˆ†æ
        """
        try:
            # 1. æŸ¥æ‰¾ç›¸ä¼¼æ±½è½¦
            similar_cars = self.vector_manager.search_similar_cars(
                query_car=car_data,
                limit=10,
                similarity_threshold=0.7
            )

            # 2. è·å–ç›¸ä¼¼æ±½è½¦çš„åˆ†æç»“æœ
            similar_analyses = []
            for car in similar_cars:
                car_id = car['metadata'].get('car_id')
                if car_id:
                    car_with_analysis = self.db_manager.get_car_with_analysis(car_id)
                    if car_with_analysis and car_with_analysis.get('analysis'):
                        similar_analyses.append({
                            'car': car_with_analysis,
                            'similarity': car['similarity']
                        })

            # 3. æ„å»ºæŸ¥è¯¢
            car_description = self.embedding_manager.create_car_description(car_data)
            query = f"""
ç”¨æˆ·è¯¢é—®å…³äºè¿™è¾†è½¦çš„ä¿¡æ¯ï¼š
{car_description}

è¯·åŸºäºç›¸ä¼¼æ¡ˆä¾‹åˆ†æï¼Œæä¾›å¸‚åœºæ´å¯Ÿå’Œå»ºè®®ã€‚
"""

            # 4. æ ¼å¼åŒ–ç›¸ä¼¼æ¡ˆä¾‹ä¿¡æ¯
            cases_context = self._format_similar_cases(similar_analyses)

            # 5. ç”Ÿæˆåˆ†æ
            analysis = self._generate_response(
                template=self.similar_cases_template,
                query=query,
                retrieved_context=cases_context
            )

            return {
                "analysis": analysis,
                "similar_cases": similar_analyses,
                "cases_count": len(similar_analyses)
            }

        except Exception as e:
            logger.error(f"Error in find_similar_cases: {e}")
            return {
                "analysis": "æ— æ³•æ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹",
                "error": str(e)
            }

    def _retrieve_for_car_analysis(self, car_data: Dict[str, Any]) -> str:
        """ä¸ºæ±½è½¦åˆ†ææ£€ç´¢ç›¸å…³ä¿¡æ¯"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            make = car_data.get('make', '')
            model = car_data.get('model', '')
            year = car_data.get('year', 0)

            search_queries = [
                f"{year} {make} {model} price analysis",
                f"{make} {model} market value",
                f"{make} reliability review",
                f"used car {make} {model} buying guide"
            ]

            retrieved_items = []

            # a) GraphRAG: å­å›¾ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            graph_context = ""
            if getattr(self, "graph_service", None):
                try:
                    graph_context = self.graph_service.context_for_car(car_data) or ""
                except Exception as e:
                    logger.warning(f"Graph context retrieval failed: {e}")

            for query in search_queries:
                # æœç´¢çŸ¥è¯†åº“
                knowledge_results = self.vector_manager.search_knowledge(
                    query_text=query,
                    limit=3
                )

                # æœç´¢åˆ†æç»“æœ
                analysis_results = self.vector_manager.search_similar_analyses(
                    query_text=query,
                    limit=3
                )

                retrieved_items.extend(knowledge_results)
                retrieved_items.extend(analysis_results)

            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ + æ‹¼æ¥å›¾ä¸Šä¸‹æ–‡
            vect_text = self._format_retrieved_info(retrieved_items)
            if graph_context:
                return f"[Graph Context]\n{graph_context}\n\n[Vector Context]\n{vect_text}"
            return vect_text

        except Exception as e:
            logger.error(f"Error retrieving for car analysis: {e}")
            return "æ— å¯ç”¨å‚è€ƒä¿¡æ¯"

    def _retrieve_for_question(self, question: str) -> str:
        """ä¸ºé—®é¢˜å›ç­”æ£€ç´¢ç›¸å…³ä¿¡æ¯"""
        try:
            # è·¨é›†åˆæœç´¢
            search_results = self.vector_manager.semantic_search(
                query_text=question,
                collections=["knowledge", "analyses", "cars"],
                limit=5
            )

            retrieved_items = []
            for collection, items in search_results.items():
                retrieved_items.extend(items)

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            retrieved_items.sort(key=lambda x: x['similarity'], reverse=True)

            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
            return self._format_retrieved_info(retrieved_items[:10])

        except Exception as e:
            logger.error(f"Error retrieving for question: {e}")
            return "æ— å¯ç”¨å‚è€ƒä¿¡æ¯"

    def _generate_response(self,
                          template: ChatPromptTemplate,
                          query: str,
                          retrieved_context: str) -> str:
        """ç”ŸæˆRAGå›å¤"""
        try:
            # æ„å»ºé“¾
            chain = template | self.llm | StrOutputParser()

            # ç”Ÿæˆå›å¤
            response = chain.invoke({
                "query": query,
                "retrieved_context": retrieved_context
            })

            return response

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ã€‚"

    def _format_retrieved_info(self, retrieved_items: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¿¡æ¯"""
        if not retrieved_items:
            return "æ— ç›¸å…³å‚è€ƒä¿¡æ¯"

        formatted_parts = []
        for i, item in enumerate(retrieved_items, 1):
            metadata = item.get('metadata', {})
            document = item.get('document', '')
            similarity = item.get('similarity', 0)

            # æ ¹æ®ä¸åŒç±»å‹æ ¼å¼åŒ–
            item_type = metadata.get('type', 'unknown')

            if item_type == 'knowledge':
                title = metadata.get('title', f'å‚è€ƒä¿¡æ¯ {i}')
                formatted_parts.append(f"ã€{title}ã€‘(ç›¸ä¼¼åº¦: {similarity:.2f})\n{document}\n")

            elif item_type == 'analysis_result':
                car_id = metadata.get('car_id', 'unknown')
                score = metadata.get('rule_based_score', 'N/A')
                formatted_parts.append(f"ã€åˆ†ææ¡ˆä¾‹ {car_id}ã€‘(ç›¸ä¼¼åº¦: {similarity:.2f}, è¯„åˆ†: {score})\n{document}\n")

            elif item_type == 'car_data':
                make = metadata.get('make', '')
                model = metadata.get('model', '')
                year = metadata.get('year', '')
                formatted_parts.append(f"ã€{year} {make} {model}ã€‘(ç›¸ä¼¼åº¦: {similarity:.2f})\n{document}\n")

            else:
                formatted_parts.append(f"ã€å‚è€ƒä¿¡æ¯ {i}ã€‘(ç›¸ä¼¼åº¦: {similarity:.2f})\n{document}\n")

        return "\n".join(formatted_parts)

    def _format_similar_cases(self, similar_analyses: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ç›¸ä¼¼æ¡ˆä¾‹"""
        if not similar_analyses:
            return "æ— ç›¸ä¼¼æ¡ˆä¾‹"

        formatted_cases = []
        for i, case in enumerate(similar_analyses, 1):
            car_info = case['car']
            analysis = car_info.get('analysis', {})
            similarity = case['similarity']

            car_desc = f"{car_info.get('year')} {car_info.get('make')} {car_info.get('model')}"
            price_paid = car_info.get('price_paid', 0)
            rule_score = analysis.get('rule_based_score', 'N/A')
            llm_score = analysis.get('llm_score', 'N/A')
            verdict = analysis.get('rule_based_verdict', 'N/A')

            case_text = f"""
æ¡ˆä¾‹ {i}: {car_desc} (ç›¸ä¼¼åº¦: {similarity:.2f})
- æˆäº¤ä»·: ${price_paid:,.0f}
- è§„åˆ™è¯„åˆ†: {rule_score}/100
- LLMè¯„åˆ†: {llm_score}/100
- è¯„ä»·: {verdict}
- æ¨ç†: {analysis.get('llm_reasoning', 'N/A')[:200]}...
"""
            formatted_cases.append(case_text)

        return "\n".join(formatted_cases)

    def _calculate_confidence(self, retrieved_info: str) -> float:
        """è®¡ç®—å›å¤çš„ç½®ä¿¡åº¦"""
        try:
            # åŸºäºæ£€ç´¢ä¿¡æ¯çš„è´¨é‡è®¡ç®—ç½®ä¿¡åº¦
            if not retrieved_info or retrieved_info == "æ— ç›¸å…³å‚è€ƒä¿¡æ¯":
                return 0.1

            # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
            lines = retrieved_info.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]

            if len(non_empty_lines) >= 10:
                return 0.9
            elif len(non_empty_lines) >= 5:
                return 0.7
            elif len(non_empty_lines) >= 2:
                return 0.5
            else:
                return 0.3

        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.1

    def _save_user_query(self,
                        query: str,
                        retrieved_info: str,
                        response: str,
                        context: Dict[str, Any] = None):
        """ä¿å­˜ç”¨æˆ·æŸ¥è¯¢ä»¥æ”¹è¿›ç³»ç»Ÿ"""
        try:
            # è¿™é‡Œå¯ä»¥ä¿å­˜åˆ°æ•°æ®åº“ç”¨äºåˆ†æå’Œæ”¹è¿›
            # æš‚æ—¶åªè®°å½•æ—¥å¿—
            logger.info(f"User query logged: {query[:100]}...")

        except Exception as e:
            logger.error(f"Error saving user query: {e}")

    # =============== æ•°æ®åŒæ­¥æ–¹æ³• ===============

    def sync_car_to_vector_store(self, car_id: int) -> bool:
        """å°†æ±½è½¦æ•°æ®åŒæ­¥åˆ°å‘é‡å­˜å‚¨"""
        try:
            car_data = self.db_manager.get_car(car_id)
            if car_data:
                return self.vector_manager.add_car(car_id, car_data)
            return False

        except Exception as e:
            logger.error(f"Error syncing car to vector store: {e}")
            return False

    def sync_analysis_to_vector_store(self, analysis_id: int, car_id: int) -> bool:
        """å°†åˆ†æç»“æœåŒæ­¥åˆ°å‘é‡å­˜å‚¨"""
        try:
            car_with_analysis = self.db_manager.get_car_with_analysis(car_id)
            if car_with_analysis and car_with_analysis.get('analysis'):
                analysis_data = car_with_analysis['analysis']
                return self.vector_manager.add_analysis(analysis_id, car_id, analysis_data)
            return False

        except Exception as e:
            logger.error(f"Error syncing analysis to vector store: {e}")
            return False

    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        try:
            db_stats = self.db_manager.get_stats()
            vector_stats = self.vector_manager.get_collection_stats()

            return {
                "database_stats": db_stats,
                "vector_store_stats": vector_stats,
                "embedding_info": self.embedding_manager.get_embedding_info(),
                "rag_system_status": "healthy"
            }

        except Exception as e:
            logger.error(f"Error getting system stats: {e}")
            return {"error": str(e)}
